"""
analyse_categorielle.py
Analyses des variables cat√©gorielles dans les donn√©es CSV.
Inclut distributions, associations, et tests de chi-carr√©.
Version optimis√©e pour √©viter les blocages.
"""
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import chi2_contingency
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

# Configuration
EXTRACTED_CSV_DIR = './extracted_csv'
RESULTS_DIR = './exploration_results'
CATEGORICAL_DIR = 'analyses_categorielles'

# Param√®tres d'optimisation pour √©viter les blocages
MAX_CATEGORIES_FOR_VISUALIZATION = 20
MAX_CATEGORICAL_PAIRS = 10  # Limite le nombre de paires pour les tests chi-carr√©
MAX_NUMERICAL_PAIRS = 15    # Limite le nombre de paires pour les tests ANOVA
SAMPLE_SIZE_FOR_TESTS = 5000  # √âchantillon pour les tests statistiques

def create_categorical_directory():
    """Cr√©e le dossier pour les analyses cat√©gorielles."""
    os.makedirs(CATEGORICAL_DIR, exist_ok=True)

def detect_categorical_columns(df, max_unique_ratio=0.1):
    """D√©tecte les colonnes cat√©gorielles avec des limites de s√©curit√©."""
    categorical_columns = []
    
    for col in df.columns:
        # Colonnes de type object (string)
        if df[col].dtype == 'object':
            # Limiter le nombre de valeurs uniques pour √©viter les blocages
            if df[col].nunique() <= MAX_CATEGORIES_FOR_VISUALIZATION:
                categorical_columns.append(col)
        # Colonnes num√©riques avec peu de valeurs uniques
        elif df[col].dtype in ['int64', 'float64']:
            unique_ratio = df[col].nunique() / len(df[col].dropna())
            if unique_ratio <= max_unique_ratio and df[col].nunique() <= 50:
                categorical_columns.append(col)
    
    return categorical_columns

def analyze_categorical_distribution(df, col, filename):
    """Analyse la distribution d'une variable cat√©gorielle avec optimisations."""
    # Nettoyer les donn√©es
    clean_data = df[col].dropna()
    
    if len(clean_data) == 0:
        return None
    
    # Calculer les statistiques
    value_counts = clean_data.value_counts()
    value_counts_normalized = clean_data.value_counts(normalize=True)
    
    # Statistiques de base
    stats = {
        'total_values': len(clean_data),
        'unique_values': len(value_counts),
        'missing_values': df[col].isna().sum(),
        'missing_percentage': (df[col].isna().sum() / len(df)) * 100,
        'most_common': value_counts.index[0],
        'most_common_count': value_counts.iloc[0],
        'most_common_percentage': value_counts_normalized.iloc[0] * 100,
        'least_common': value_counts.index[-1],
        'least_common_count': value_counts.iloc[-1],
        'least_common_percentage': value_counts_normalized.iloc[-1] * 100
    }
    
    # Visualisation optimis√©e
    try:
        fig, axes = plt.subplots(1, 2, figsize=(15, 6))
        
        # 1. Graphique en barres (limit√© aux top 10)
        top_values = value_counts.head(10)
        axes[0].bar(range(len(top_values)), top_values.values, color='skyblue')
        axes[0].set_title(f'Distribution de {col} (Top 10)')
        axes[0].set_xlabel('Valeurs')
        axes[0].set_ylabel('Fr√©quence')
        axes[0].set_xticks(range(len(top_values)))
        axes[0].set_xticklabels(top_values.index, rotation=45, ha='right')
        
        # 2. Graphique en camembert (limit√© aux top 5)
        if len(value_counts) <= 10:
            axes[1].pie(value_counts.values, labels=value_counts.index, autopct='%1.1f%%')
            axes[1].set_title(f'R√©partition de {col}')
        else:
            # Grouper les valeurs moins fr√©quentes
            top_5 = value_counts.head(5)
            others = value_counts.iloc[5:].sum()
            pie_data = pd.concat([top_5, pd.Series([others], index=['Autres'])])
            axes[1].pie(pie_data.values, labels=pie_data.index, autopct='%1.1f%%')
            axes[1].set_title(f'R√©partition de {col} (Top 5 + Autres)')
        
        plt.tight_layout()
        plt.savefig(os.path.join(CATEGORICAL_DIR, f'{filename}_{col}_distribution.png'), dpi=300)
        plt.close()
    except Exception as e:
        print(f"‚ö†Ô∏è Erreur lors de la cr√©ation de la visualisation pour {col}: {e}")
    
    return {
        'stats': stats,
        'value_counts': value_counts,
        'value_counts_normalized': value_counts_normalized
    }

def analyze_categorical_associations(df, cat_columns, filename):
    """Analyse les associations entre variables cat√©gorielles avec limites."""
    if len(cat_columns) < 2:
        return None
    
    # Limiter le nombre de paires pour √©viter les blocages
    max_pairs = min(MAX_CATEGORICAL_PAIRS, len(cat_columns) * (len(cat_columns) - 1) // 2)
    associations = {}
    pair_count = 0
    
    # √âchantillonner les donn√©es pour les tests
    df_sample = df[cat_columns].dropna().sample(n=min(SAMPLE_SIZE_FOR_TESTS, len(df)), random_state=42)
    
    for i, col1 in enumerate(cat_columns):
        if pair_count >= max_pairs:
            break
        for j, col2 in enumerate(cat_columns[i+1:], i+1):
            if pair_count >= max_pairs:
                break
            
            # Cr√©er la table de contingence
            try:
                contingency_table = pd.crosstab(df_sample[col1], df_sample[col2])
                
                # V√©rifier que la table n'est pas trop grande
                if contingency_table.shape[0] > 20 or contingency_table.shape[1] > 20:
                    print(f"‚ö†Ô∏è Table de contingence trop grande pour {col1} vs {col2}, ignor√©e")
                    continue
                
                # Test de chi-carr√©
                chi2, p_value, dof, expected = chi2_contingency(contingency_table)
                
                # Coefficient de contingence de Cram√©r
                n = contingency_table.sum().sum()
                cramer_v = np.sqrt(chi2 / (n * (min(contingency_table.shape) - 1)))
                
                associations[f"{col1}_vs_{col2}"] = {
                    'contingency_table': contingency_table,
                    'chi2_statistic': chi2,
                    'p_value': p_value,
                    'degrees_of_freedom': dof,
                    'cramer_v': cramer_v,
                    'significant': p_value < 0.05,
                    'strength': 'forte' if cramer_v > 0.3 else 'mod√©r√©e' if cramer_v > 0.1 else 'faible'
                }
                pair_count += 1
                
            except Exception as e:
                print(f"‚ö†Ô∏è Erreur lors du test chi-carr√© pour {col1} vs {col2}: {e}")
    
    # Visualisation des associations (limit√©e)
    if associations:
        try:
            n_associations = len(associations)
            n_cols = min(3, n_associations)
            n_rows = (n_associations + n_cols - 1) // n_cols
            
            fig, axes = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 4*n_rows))
            if n_rows == 1:
                axes = [axes] if n_cols == 1 else axes
            else:
                axes = axes.flatten()
            
            for idx, (assoc_name, assoc_data) in enumerate(associations.items()):
                if idx < len(axes):
                    # Heatmap de la table de contingence
                    sns.heatmap(assoc_data['contingency_table'], 
                               annot=True, fmt='d', cmap='Blues', ax=axes[idx])
                    axes[idx].set_title(f'{assoc_name}\nœá¬≤={assoc_data["chi2_statistic"]:.2f}, p={assoc_data["p_value"]:.4f}')
            
            # Masquer les axes vides
            for idx in range(n_associations, len(axes)):
                axes[idx].set_visible(False)
            
            plt.tight_layout()
            plt.savefig(os.path.join(CATEGORICAL_DIR, f'{filename}_associations.png'), dpi=300)
            plt.close()
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur lors de la cr√©ation des visualisations d'associations: {e}")
    
    return associations

def analyze_categorical_numerical_relationships(df, cat_columns, num_columns, filename):
    """Analyse les relations entre variables cat√©gorielles et num√©riques avec limites."""
    if len(cat_columns) == 0 or len(num_columns) == 0:
        return None
    
    # Limiter le nombre de paires
    max_pairs = min(MAX_NUMERICAL_PAIRS, len(cat_columns) * len(num_columns))
    relationships = {}
    pair_count = 0
    
    # √âchantillonner les donn√©es
    df_sample = df[cat_columns + list(num_columns)].dropna().sample(n=min(SAMPLE_SIZE_FOR_TESTS, len(df)), random_state=42)
    
    for cat_col in cat_columns:
        if pair_count >= max_pairs:
            break
        for num_col in num_columns:
            if pair_count >= max_pairs:
                break
            
            try:
                # Statistiques par cat√©gorie
                grouped_stats = df_sample.groupby(cat_col)[num_col].agg(['count', 'mean', 'std', 'min', 'max']).reset_index()
                
                # Test ANOVA (si plus de 2 cat√©gories)
                categories = df_sample[cat_col].dropna().unique()
                if len(categories) >= 2 and len(categories) <= 20:  # Limite le nombre de cat√©gories
                    from scipy.stats import f_oneway
                    groups = [df_sample[df_sample[cat_col] == cat][num_col].dropna() for cat in categories]
                    # Filtrer les groupes vides
                    groups = [g for g in groups if len(g) > 0]
                    if len(groups) >= 2:
                        f_stat, p_value = f_oneway(*groups)
                        
                        relationships[f"{cat_col}_vs_{num_col}"] = {
                            'grouped_stats': grouped_stats,
                            'f_statistic': f_stat,
                            'p_value': p_value,
                            'significant': p_value < 0.05,
                            'n_categories': len(categories)
                        }
                        pair_count += 1
                        
            except Exception as e:
                print(f"‚ö†Ô∏è Erreur lors du test ANOVA pour {cat_col} vs {num_col}: {e}")
    
    # Visualisations (limit√©es)
    if relationships:
        try:
            n_relationships = len(relationships)
            n_cols = min(2, n_relationships)
            n_rows = (n_relationships + n_cols - 1) // n_cols
            
            fig, axes = plt.subplots(n_rows, n_cols, figsize=(8*n_cols, 6*n_rows))
            if n_rows == 1:
                axes = [axes] if n_cols == 1 else axes
            else:
                axes = axes.flatten()
            
            for idx, (rel_name, rel_data) in enumerate(relationships.items()):
                if idx < len(axes):
                    # Box plot
                    cat_col, num_col = rel_name.split('_vs_')
                    df_clean = df_sample[[cat_col, num_col]].dropna()
                    
                    # Limiter le nombre de cat√©gories pour la lisibilit√©
                    if len(df_clean[cat_col].unique()) > 10:
                        top_categories = df_clean[cat_col].value_counts().head(10).index
                        df_clean = df_clean[df_clean[cat_col].isin(top_categories)]
                    
                    sns.boxplot(data=df_clean, x=cat_col, y=num_col, ax=axes[idx])
                    axes[idx].set_title(f'{rel_name}\nF={rel_data["f_statistic"]:.2f}, p={rel_data["p_value"]:.4f}')
                    axes[idx].tick_params(axis='x', rotation=45)
            
            # Masquer les axes vides
            for idx in range(n_relationships, len(axes)):
                axes[idx].set_visible(False)
            
            plt.tight_layout()
            plt.savefig(os.path.join(CATEGORICAL_DIR, f'{filename}_cat_num_relationships.png'), dpi=300)
            plt.close()
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur lors de la cr√©ation des visualisations de relations: {e}")
    
    return relationships

def generate_categorical_report(df, filename):
    """G√©n√®re un rapport complet d'analyse cat√©gorielle avec optimisations."""
    print(f"üìä Analyse cat√©gorielle de {filename}...")
    
    # D√©tecter les colonnes cat√©gorielles
    categorical_columns = detect_categorical_columns(df)
    
    if not categorical_columns:
        print(f"  ‚ö†Ô∏è Aucune variable cat√©gorielle d√©tect√©e dans {filename}")
        return False
    
    print(f"  üìã Variables cat√©gorielles d√©tect√©es : {len(categorical_columns)}")
    
    # Colonnes num√©riques pour les relations
    numerical_columns = df.select_dtypes(include=[np.number]).columns
    
    # Analyses avec gestion d'erreurs
    distributions = {}
    associations = None
    relationships = None
    
    # Distributions individuelles
    for col in categorical_columns:
        try:
            distributions[col] = analyze_categorical_distribution(df, col, filename)
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur lors de l'analyse de la distribution de {col}: {e}")
    
    # Associations entre variables cat√©gorielles
    if len(categorical_columns) >= 2:
        try:
            associations = analyze_categorical_associations(df, categorical_columns, filename)
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur lors de l'analyse des associations: {e}")
    
    # Relations avec variables num√©riques
    if len(numerical_columns) > 0:
        try:
            relationships = analyze_categorical_numerical_relationships(df, categorical_columns, numerical_columns, filename)
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur lors de l'analyse des relations: {e}")
    
    # G√©n√©rer le rapport texte
    try:
        report_path = os.path.join(CATEGORICAL_DIR, f'{filename}_categorical_report.txt')
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(f"RAPPORT D'ANALYSE CAT√âGORIELLE - {filename}\n")
            f.write("=" * 50 + "\n\n")
            
            f.write(f"üìä INFORMATIONS G√âN√âRALES\n")
            f.write(f"Nombre de variables cat√©gorielles : {len(categorical_columns)}\n")
            f.write(f"Variables cat√©gorielles : {', '.join(categorical_columns)}\n\n")
            
            # Distributions
            f.write(f"üìà DISTRIBUTIONS INDIVIDUELLES\n")
            for col, dist_data in distributions.items():
                if dist_data:
                    stats = dist_data['stats']
                    f.write(f"\n{col}:\n")
                    f.write(f"  - Total de valeurs : {stats['total_values']}\n")
                    f.write(f"  - Valeurs uniques : {stats['unique_values']}\n")
                    f.write(f"  - Valeurs manquantes : {stats['missing_values']} ({stats['missing_percentage']:.1f}%)\n")
                    f.write(f"  - Valeur la plus fr√©quente : {stats['most_common']} ({stats['most_common_count']} fois, {stats['most_common_percentage']:.1f}%)\n")
                    f.write(f"  - Valeur la moins fr√©quente : {stats['least_common']} ({stats['least_common_count']} fois, {stats['least_common_percentage']:.1f}%)\n")
                    
                    # Top 5 valeurs
                    top_5 = dist_data['value_counts'].head(5)
                    f.write(f"  - Top 5 valeurs :\n")
                    for val, count in top_5.items():
                        percentage = (count / stats['total_values']) * 100
                        f.write(f"    * {val}: {count} ({percentage:.1f}%)\n")
            
            # Associations
            if associations:
                f.write(f"\nüîó ASSOCIATIONS ENTRE VARIABLES CAT√âGORIELLES\n")
                for assoc_name, assoc_data in associations.items():
                    f.write(f"\n{assoc_name}:\n")
                    f.write(f"  - Test de chi-carr√© : œá¬≤ = {assoc_data['chi2_statistic']:.4f}\n")
                    f.write(f"  - P-valeur : {assoc_data['p_value']:.4f}\n")
                    f.write(f"  - Significatif : {'Oui' if assoc_data['significant'] else 'Non'}\n")
                    f.write(f"  - Coefficient de Cram√©r : {assoc_data['cramer_v']:.4f}\n")
                    f.write(f"  - Force de l'association : {assoc_data['strength']}\n")
            
            # Relations avec variables num√©riques
            if relationships:
                f.write(f"\nüìä RELATIONS AVEC VARIABLES NUM√âRIQUES\n")
                for rel_name, rel_data in relationships.items():
                    f.write(f"\n{rel_name}:\n")
                    f.write(f"  - Test ANOVA : F = {rel_data['f_statistic']:.4f}\n")
                    f.write(f"  - P-valeur : {rel_data['p_value']:.4f}\n")
                    f.write(f"  - Significatif : {'Oui' if rel_data['significant'] else 'Non'}\n")
                    f.write(f"  - Nombre de cat√©gories : {rel_data['n_categories']}\n")
                    
                    # Statistiques par groupe
                    f.write(f"  - Statistiques par cat√©gorie :\n")
                    for _, row in rel_data['grouped_stats'].iterrows():
                        f.write(f"    * {row.iloc[0]}: n={row['count']}, moy={row['mean']:.2f}, std={row['std']:.2f}\n")
            
            f.write(f"\nüìÅ FICHIERS G√âN√âR√âS :\n")
            f.write(f"  - Distributions : {filename}_*_distribution.png\n")
            if associations:
                f.write(f"  - Associations : {filename}_associations.png\n")
            if relationships:
                f.write(f"  - Relations : {filename}_cat_num_relationships.png\n")
            f.write(f"  - Rapport : {filename}_categorical_report.txt\n")
        
        print(f"‚úÖ Rapport cat√©goriel g√©n√©r√© : {report_path}")
        return True
        
    except Exception as e:
        print(f"‚ùå Erreur lors de la g√©n√©ration du rapport: {e}")
        return False

def main():
    """Fonction principale pour les analyses cat√©gorielles."""
    print("üìä D√©but des analyses cat√©gorielles...")
    
    # Cr√©er le dossier de r√©sultats
    create_categorical_directory()
    
    # Lister les fichiers CSV
    extracted_dir = Path(EXTRACTED_CSV_DIR)
    if not extracted_dir.exists():
        print(f"‚ùå Le dossier {EXTRACTED_CSV_DIR} n'existe pas.")
        print("üí° Veuillez d'abord ex√©cuter extract.py pour extraire les fichiers compress√©s.")
        print("   Commande : python ./etl_steps/extract.py")
        return
    
    csv_files = list(extracted_dir.rglob("*.csv"))
    
    if not csv_files:
        print("‚ùå Aucun fichier CSV trouv√©.")
        print("üí° Veuillez d'abord ex√©cuter extract.py pour extraire les fichiers compress√©s.")
        print("   Commande : python ./etl_steps/extract.py")
        return
    
    print(f"üìÅ {len(csv_files)} fichier(s) CSV trouv√©(s)")
    
    # Analyser chaque fichier
    successful_analyses = 0
    for csv_path in csv_files:
        try:
            print(f"\nüìä Traitement de {os.path.basename(csv_path)}...")
            # Charger un √©chantillon pour √©viter les blocages
            df = pd.read_csv(csv_path, nrows=10000)
            filename = os.path.splitext(os.path.basename(csv_path))[0]
            
            if generate_categorical_report(df, filename):
                successful_analyses += 1
        except Exception as e:
            print(f"‚ùå Erreur lors de l'analyse de {csv_path}: {e}")
    
    print(f"\n‚úÖ Analyses cat√©gorielles termin√©es !")
    print(f"   - Fichiers analys√©s : {successful_analyses}/{len(csv_files)}")
    print(f"   - R√©sultats dans : {CATEGORICAL_DIR}")

if __name__ == "__main__":
    main() 